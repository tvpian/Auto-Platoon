{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 23:47:29.585979: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-14 23:47:39.364471: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-14 23:47:39.425954: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-14 23:48:03.064383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from mediapipe.tasks.python import vision\n",
    "import mediapipe as mp\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectDectector_model_filepath = \"model/object_detector/efficientdet_lite0_uint8.tflite\"\n",
    "video_filepath = \"dataset/video_cars.mp4\"\n",
    "embedder_model_filepath = \"model/embedder/mobilenet_v3_small_075_224_embedder.tflite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObjecttDetectorOptions = mp.tasks.vision.ObjectDetectorOptions(\n",
    "    base_options=mp.tasks.BaseOptions(model_asset_path=objectDectector_model_filepath),\n",
    "    max_results=50,\n",
    "    score_threshold=0.4,\n",
    "    running_mode=mp.tasks.vision.RunningMode.VIDEO\n",
    ")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS:  60\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_filepath)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "print(\"FPS: \",fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up video writer for testing the object detector\n",
    "# We need to set resolutions.\n",
    "# so, convert them from float to integer.\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "   \n",
    "size = (frame_width, frame_height)\n",
    "   \n",
    "# Below VideoWriter object will create\n",
    "# a frame of above defined The output \n",
    "# # is stored in 'filename.avi' file.\n",
    "# videoWriter = cv2.VideoWriter('objDetect_video_cars_test.avi', \n",
    "#                          cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "#                          30, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not able to read the frame\n",
      "time taken :  0.09556216746357879\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "frame_idx = 0\n",
    "all_detections=defaultdict(list)\n",
    "avg_processing_time = 0\n",
    "with mp.tasks.vision.ObjectDetector.create_from_options(ObjecttDetectorOptions) as detector:\n",
    "    while cap.isOpened():\n",
    "        ret, img = cap.read()\n",
    "        if ret == True:\n",
    "\n",
    "            frame_idx+=1\n",
    "            # Calculate the timestamp of the current frame\n",
    "            frame_timestamp_ms = int(1000 * frame_idx / fps)\n",
    "            # print(frame_timestamp_ms)\n",
    "            time_start = time.perf_counter()\n",
    "            # Convert the frame received from OpenCV to a MediaPipe’s Image object.\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img)\n",
    "\n",
    "            # Perform object detection on the video frame.\n",
    "            detection_results = detector.detect_for_video(mp_image, frame_timestamp_ms)\n",
    "            # print(\"time taken : \",time.perf_counter()-time_start)\n",
    "            # print(\"Detections:\",len(detection_results.detections))\n",
    "            # print(\"--------------\")\n",
    "            for det_res in detection_results.detections:\n",
    "                x = det_res.bounding_box.origin_x\n",
    "                y = det_res.bounding_box.origin_y\n",
    "                width = det_res.bounding_box.width\n",
    "                height = det_res.bounding_box.height\n",
    "                all_detections[frame_idx].append((x,y,x+width,y+height))\n",
    "                # img = cv2.rectangle(img, (x,y),(x+width,y+height),color=(0,255,0),thickness=2)\n",
    "            # cv2.imwrite(\"box_img.png\",img)\n",
    "            # videoWriter.write(img)\n",
    "            # break\n",
    "            time_processing = time.perf_counter()-time_start\n",
    "            avg_processing_time += time_processing\n",
    "        else:\n",
    "            print(\"Not able to read the frame\")\n",
    "            break\n",
    "cap.release()\n",
    "# videoWriter.release()\n",
    "print(\"time taken : \",avg_processing_time/frame_idx)\n",
    "# print(\"Detections:\",len(detection_results.detections))\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Detection(bounding_box=BoundingBox(origin_x=2269, origin_y=1173, width=497, height=388), categories=[Category(index=None, score=0.62890625, display_name=None, category_name='car')], keypoints=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_results.detections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings:\n",
    "# Create options for Image Embedder\n",
    "embedder_base_options = mp.tasks.BaseOptions(model_asset_path=embedder_model_filepath)\n",
    "l2_normalize = True #@param {type:\"boolean\"}\n",
    "quantize = True #@param {type:\"boolean\"}\n",
    "options = vision.ImageEmbedderOptions(\n",
    "    base_options=embedder_base_options, l2_normalize=l2_normalize, quantize=quantize,\n",
    "    running_mode= mp.tasks.vision.RunningMode.VIDEO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS:  60\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_filepath)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "print(\"FPS: \",fps)\n",
    "\n",
    "# setting up video writer for testing the object detector\n",
    "# We need to set resolutions.\n",
    "# so, convert them from float to integer.\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "   \n",
    "size = (frame_width, frame_height)\n",
    "   \n",
    "# Below VideoWriter object will create\n",
    "# a frame of above defined The output \n",
    "# # is stored in 'filename.avi' file.\n",
    "videoWriter = cv2.VideoWriter('objEmbed_video_cars_test.avi', \n",
    "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                         10, size)\n",
    "output_image_dir = \"images\"\n",
    "os.makedirs(output_image_dir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not able to read the frame\n",
      "time taken :  6.232323939660664\n"
     ]
    }
   ],
   "source": [
    "# Create Image Embedder\n",
    "frame_idx = 0\n",
    "prev_embeddings = [] # prev frame as 1, next frame as 2\n",
    "next_embeddings = []\n",
    "crop_idx = 0\n",
    "frame_interval = 10 # after how many frames check similarity\n",
    "avg_processing_time = 0\n",
    "car_idx = 0 #car counts detected cars in the video, same car will be not counted more than 1\n",
    "matching_thresh = 0.7\n",
    "\n",
    "with vision.ImageEmbedder.create_from_options(options) as embedder:\n",
    "   while cap.isOpened():\n",
    "        ret, img = cap.read()\n",
    "        if ret == True:\n",
    "\n",
    "            frame_idx+=1\n",
    "            if (frame_idx-1)%frame_interval==0:\n",
    "\n",
    "                time_start = time.perf_counter()\n",
    "                if len(prev_embeddings)==0:\n",
    "                    embed_arr = []\n",
    "                    car_idx_arr = []\n",
    "                    # take first frame as the reference for starting tracking\n",
    "                    for x1,y1,x2,y2 in all_detections[1]:\n",
    "                        car_idx+=1\n",
    "                        img_crop = img[y1:y2,x1:x2,:].astype(np.uint8)\n",
    "                        crop_idx+=1\n",
    "                        # Calculate the timestamp of the current frame\n",
    "                        frame_timestamp_ms = int(1000 * crop_idx / fps)\n",
    "                        # Convert the frame received from OpenCV to a MediaPipe’s Image object.\n",
    "                        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_crop)\n",
    "\n",
    "                        # Perform object detection on the video frame.\n",
    "                        embedding_result = embedder.embed_for_video(mp_image, frame_timestamp_ms)\n",
    "\n",
    "                        embed_arr.append(embedding_result.embeddings[0])\n",
    "                        car_idx_arr.append(car_idx)\n",
    "                        # print(int(frame_idx//frame_interval),embedding_result)\n",
    "                        prev_embeddings.append((img_crop,embedding_result,car_idx,(x1,y1,x2,y2)))\n",
    "                    # print(embed_arr)\n",
    "                    # break\n",
    "                else:\n",
    "                    for x1,y1,x2,y2 in all_detections[frame_idx]:\n",
    "                        img_crop = img[y1:y2,x1:x2,:].astype(np.uint8)\n",
    "                        crop_idx+=1\n",
    "                        # Calculate the timestamp of the current frame\n",
    "                        frame_timestamp_ms = int(1000 * crop_idx / fps)\n",
    "                        # Convert the frame received from OpenCV to a MediaPipe’s Image object.\n",
    "                        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_crop)\n",
    "\n",
    "                        # Perform object detection on the video frame.\n",
    "                        embedding_result = embedder.embed_for_video(mp_image, frame_timestamp_ms)\n",
    "                        em1 = embedding_result.embeddings[0]\n",
    "                        best_match = 0\n",
    "                        new_car_idx = None\n",
    "                        for im_cr_prev, embed_prev, c_idx, coords in prev_embeddings:\n",
    "                            em2 = embed_prev.embeddings[0]\n",
    "                            similarity = vision.ImageEmbedder.cosine_similarity(\n",
    "                                em1,\n",
    "                                em2)\n",
    "\n",
    "                            if similarity>matching_thresh and similarity>best_match:\n",
    "                                best_match = similarity\n",
    "                                new_car_idx = c_idx\n",
    "                        if new_car_idx is None:\n",
    "                            car_idx+=1\n",
    "                            new_car_idx = car_idx\n",
    "                        # print(int(frame_idx//frame_interval),embedding_result)\n",
    "                        next_embeddings.append((img_crop,embedding_result,new_car_idx,(x1,y1,x2,y2)))\n",
    "                    prev_embeddings = next_embeddings.copy()\n",
    "                    next_embeddings = []\n",
    "     \n",
    "                for im_cr_prev, embed_prev, c_idx, coords in prev_embeddings:\n",
    "                    x1 = coords[0]\n",
    "                    y1 = coords[1]\n",
    "                    x2 = coords[2]\n",
    "                    y2 = coords[3]\n",
    "                    img = cv2.putText(img, f'{c_idx}', org=(x1,y1),\n",
    "                                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                        fontScale=3,\n",
    "                                        color=(255,0,0),\n",
    "                                        thickness=2,\n",
    "                                        lineType=cv2.LINE_AA)\n",
    "                    img = cv2.rectangle(img, (x1,y1),(x2,y2),color=(0,255,0),thickness=5)\n",
    "                cv2.imwrite(os.path.join(output_image_dir,f\"image_{frame_idx}.png\"),img)\n",
    "                videoWriter.write(img)\n",
    "                try:    \n",
    "                    avg_processing_time+=(time.perf_counter()-time_start)/((frame_idx-1)//frame_interval)\n",
    "                except ZeroDivisionError as ZE:\n",
    "                    avg_processing_time+=(time.perf_counter()-time_start)\n",
    "\n",
    "        else:\n",
    "            print(\"Not able to read the frame\")\n",
    "            break\n",
    "cap.release()\n",
    "videoWriter.release()\n",
    "print(\"time taken : \",avg_processing_time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Kalman filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filterpy.kalman import KalmanFilter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS:  60\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_filepath)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "print(\"FPS: \",fps)\n",
    "\n",
    "# setting up video writer for testing the object detector\n",
    "# We need to set resolutions.\n",
    "# so, convert them from float to integer.\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "   \n",
    "size = (frame_width, frame_height)\n",
    "   \n",
    "# Below VideoWriter object will create\n",
    "# a frame of above defined The output \n",
    "# # is stored in 'filename.avi' file.\n",
    "videoWriter = cv2.VideoWriter('objEmbed_video_cars_test.avi', \n",
    "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                         10, size)\n",
    "output_image_dir = \"images\"\n",
    "os.makedirs(output_image_dir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings:\n",
    "# Create options for Image Embedder\n",
    "embedder_base_options = mp.tasks.BaseOptions(model_asset_path=embedder_model_filepath)\n",
    "l2_normalize = True #@param {type:\"boolean\"}\n",
    "quantize = True #@param {type:\"boolean\"}\n",
    "emb_options = vision.ImageEmbedderOptions(\n",
    "    base_options=embedder_base_options, l2_normalize=l2_normalize, quantize=quantize,\n",
    "    running_mode= mp.tasks.vision.RunningMode.VIDEO)\n",
    "\n",
    "ObjecttDetectorOptions = mp.tasks.vision.ObjectDetectorOptions(\n",
    "    base_options=mp.tasks.BaseOptions(model_asset_path=objectDectector_model_filepath),\n",
    "    max_results=30,\n",
    "    score_threshold=0.4,\n",
    "    running_mode=mp.tasks.vision.RunningMode.VIDEO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Track():\n",
    "    def __init__(self, track_idx, bbox, features, frame_limit =10, input_st=7, output_st=4):\n",
    "        # input_st = 7\n",
    "        # output_st = 4\n",
    "        \n",
    "        self.warming = True\n",
    "        self.warming_count = 5 # that many frames needed to predict accuratly for the pose\n",
    "        self.set_kf(input_st, output_st)\n",
    "        self.kf.x[:4] = self.convert_bbox_to_z(bbox)\n",
    "\n",
    "        self.track_idx = track_idx\n",
    "        self.obj_not_seen = 0 # object not seen for certain frames count\n",
    "        self.frame_limit = frame_limit\n",
    "        self.features = features\n",
    "\n",
    "        self.time_since_update = 0\n",
    "\n",
    "    def notFound(self):\n",
    "        if self.time_since_update>self.frame_limit:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def set_kf(self, input_st, output_st):\n",
    "        self.kf = KalmanFilter(dim_x=input_st, dim_z=output_st)\n",
    "\n",
    "        # A\n",
    "        self.kf.F = np.array([[1,0,0,0,1,0,0],[0,1,0,0,0,1,0],[0,0,1,0,0,0,1],[0,0,0,1,0,0,0],  [0,0,0,0,1,0,0],[0,0,0,0,0,1,0],[0,0,0,0,0,0,1]])\n",
    "\n",
    "        # H\n",
    "        self.kf.H = np.array([[1,0,0,0,0,0,0],[0,1,0,0,0,0,0],[0,0,1,0,0,0,0],[0,0,0,1,0,0,0]])\n",
    "\n",
    "        self.kf.R[2:,2:] *= 10.\n",
    "        self.kf.P[4:,4:] *= 1000. #give high uncertainty to the unobservable initial velocities\n",
    "        self.kf.P *= 10.\n",
    "        self.kf.Q[-1,-1] *= 0.01\n",
    "        self.kf.Q[4:,4:] *= 0.01\n",
    "\n",
    "    def convert_bbox_to_z(self,bbox):\n",
    "        \"\"\"\n",
    "        Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n",
    "        [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n",
    "        the aspect ratio\n",
    "        \"\"\"\n",
    "        w = bbox[2] - bbox[0]\n",
    "        h = bbox[3] - bbox[1]\n",
    "        x = bbox[0] + w/2.\n",
    "        y = bbox[1] + h/2.\n",
    "        s = w * h    #scale is just area\n",
    "        r = w / float(h)\n",
    "        return np.array([x, y, s, r]).reshape((4, 1))\n",
    "\n",
    "    def convert_x_to_bbox(self,x):\n",
    "        w = np.sqrt(x[2] * x[3])\n",
    "        h = x[2] / w\n",
    "        return np.array([x[0]-w/2.,x[1]-h/2.,x[0]+w/2.,x[1]+h/2.]).reshape((1,4))\n",
    "    \n",
    "    def update(self,bbox, features):\n",
    "        \"\"\"\n",
    "        Updates the state vector with observed bbox.\n",
    "        \"\"\"\n",
    "        self.time_since_update = 0\n",
    "        history = []\n",
    "\n",
    "        # update features\n",
    "        prev_feature = self.features.embedding\n",
    "        new_feature = features.embedding\n",
    "        self.features.embedding = (prev_feature+new_feature)/2\n",
    "\n",
    "        self.kf.update(self.convert_bbox_to_z(bbox))\n",
    "        if self.warming:\n",
    "            self.warming_count-=1\n",
    "            if self.warming_count<=0:\n",
    "                self.warming=False\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Advances the state vector and returns the predicted bounding box estimate.\n",
    "        \"\"\"\n",
    "        if((self.kf.x[6]+self.kf.x[2])<=0):\n",
    "            self.kf.x[6] *= 0.0\n",
    "        self.kf.predict()\n",
    "        self.time_since_update += 1\n",
    "        # history.append()\n",
    "        return self.convert_x_to_bbox(self.kf.x)\n",
    "        # return history[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOU(bbox1,bbox2):\n",
    "\t\n",
    "\t# determine the (x, y)-coordinates of the intersection rectangle\n",
    "\txA = max(bbox1[0], bbox2[0])\n",
    "\tyA = max(bbox1[1], bbox2[1])\n",
    "\txB = min(bbox1[2], bbox2[2])\n",
    "\tyB = min(bbox1[3], bbox2[3])\n",
    "\t# compute the area of intersection rectangle\n",
    "\tinterArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\t# compute the area of both the prediction and ground-truth\n",
    "\t# rectangles\n",
    "\tbbox1Area = (bbox1[2] - bbox1[0] + 1) * (bbox1[3] - bbox1[1] + 1)\n",
    "\tbbox2Area = (bbox2[2] - bbox2[0] + 1) * (bbox2[3] - bbox2[1] + 1)\n",
    "\t# compute the intersection over union by taking the intersection\n",
    "\t# area and dividing it by the sum of prediction + ground-truth\n",
    "\t# areas - the interesection area\n",
    "\tiou = interArea / float(bbox1Area + bbox2Area - interArea)\n",
    "\t# return the intersection over union value\n",
    "\treturn iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken :  0.13375994990048495\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "frame_idx = 0\n",
    "# Embedding with tracking\n",
    "# Create Image Embedder\n",
    "crop_idx = 0\n",
    "\n",
    "all_detections=defaultdict(list)\n",
    "avg_processing_time = 0\n",
    "# store_images = {}\n",
    "\n",
    "with vision.ImageEmbedder.create_from_options(emb_options) as embedder:\n",
    "    with mp.tasks.vision.ObjectDetector.create_from_options(ObjecttDetectorOptions) as detector:\n",
    "        while cap.isOpened():\n",
    "            ret, img = cap.read()\n",
    "            if ret == True:\n",
    "\n",
    "                frame_idx+=1\n",
    "                # Calculate the timestamp of the current frame\n",
    "                frame_timestamp_ms = int(1000 * frame_idx / fps)\n",
    "                # print(frame_timestamp_ms)\n",
    "                time_start = time.perf_counter()\n",
    "                # Convert the frame received from OpenCV to a MediaPipe’s Image object.\n",
    "                mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img)\n",
    "\n",
    "                # Perform object detection on the video frame.\n",
    "                detection_results = detector.detect_for_video(mp_image, frame_timestamp_ms)\n",
    "                # print(\"time taken : \",time.perf_counter()-time_start)\n",
    "                # print(\"Detections:\",len(detection_results.detections))\n",
    "                # print(\"--------------\")\n",
    "                # crop_idx=0\n",
    "                for det_res in detection_results.detections:\n",
    "                    x = det_res.bounding_box.origin_x\n",
    "                    y = det_res.bounding_box.origin_y\n",
    "                    width = det_res.bounding_box.width\n",
    "                    height = det_res.bounding_box.height\n",
    "                    \n",
    "                    x1,y1,x2,y2 = (x,y,x+width,y+height)\n",
    "                    img_crop = img[y1:y2,x1:x2,:].astype(np.uint8)\n",
    "                    crop_idx+=1\n",
    "                    # Calculate the timestamp of the current frame\n",
    "                    frame_timestamp_ms = int(1000 * crop_idx / fps)\n",
    "                    # Convert the frame received from OpenCV to a MediaPipe’s Image object.\n",
    "                    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_crop)\n",
    "\n",
    "                    # Perform object detection on the video frame.\n",
    "                    embedding_result = embedder.embed_for_video(mp_image, frame_timestamp_ms)\n",
    "\n",
    "                    # embed_arr.append(embedding_result.embeddings[0])\n",
    "                    features = embedding_result.embeddings[0]\n",
    "                    all_detections[frame_idx].append(((x1,y1,x2,y2), features))\n",
    "                    # img = cv2.rectangle(img, (x,y),(x+width,y+height),color=(0,255,0),thickness=2)\n",
    "                # store_images[frame_idx] = store_images.get(frame_idx, img)\n",
    "                # cv2.imwrite(\"box_img.png\",img)\n",
    "                # videoWriter.write(img)\n",
    "                # break\n",
    "                time_processing = time.perf_counter()-time_start\n",
    "                avg_processing_time += time_processing\n",
    "            else:\n",
    "                print(\"Not able to read the frame\")\n",
    "                break\n",
    "\n",
    "            if frame_idx>200:\n",
    "                break\n",
    "cap.release()\n",
    "# videoWriter.release()\n",
    "print(\"time taken : \",avg_processing_time/frame_idx)\n",
    "# print(\"Detections:\",len(detection_results.detections))\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS:  60\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_filepath)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "print(\"FPS: \",fps)\n",
    "\n",
    "# setting up video writer for testing the object detector\n",
    "# We need to set resolutions.\n",
    "# so, convert them from float to integer.\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "size = (frame_width, frame_height)\n",
    "   \n",
    "# Below VideoWriter object will create\n",
    "# a frame of above defined The output \n",
    "# # is stored in 'filename.avi' file.\n",
    "videoWriter = cv2.VideoWriter('objEmbed_video_cars_track.avi', \n",
    "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                         10, size)\n",
    "output_image_dir = \"images\"\n",
    "os.makedirs(output_image_dir,exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* Add NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing frame :  1\n",
      "processing frame :  2\n",
      "processing frame :  3\n",
      "processing frame :  4\n",
      "processing frame :  5\n",
      "processing frame :  6\n",
      "processing frame :  7\n",
      "processing frame :  8\n",
      "processing frame :  9\n",
      "processing frame :  10\n",
      "processing frame :  11\n",
      "processing frame :  12\n",
      "processing frame :  13\n",
      "processing frame :  14\n",
      "processing frame :  15\n",
      "processing frame :  16\n",
      "processing frame :  17\n",
      "processing frame :  18\n",
      "processing frame :  19\n",
      "processing frame :  20\n",
      "processing frame :  21\n",
      "processing frame :  22\n",
      "processing frame :  23\n",
      "processing frame :  24\n",
      "processing frame :  25\n",
      "processing frame :  26\n",
      "processing frame :  27\n",
      "processing frame :  28\n",
      "processing frame :  29\n",
      "processing frame :  30\n",
      "processing frame :  31\n",
      "processing frame :  32\n",
      "processing frame :  33\n",
      "processing frame :  34\n",
      "processing frame :  35\n",
      "processing frame :  36\n",
      "processing frame :  37\n",
      "processing frame :  38\n",
      "processing frame :  39\n",
      "processing frame :  40\n",
      "processing frame :  41\n",
      "processing frame :  42\n",
      "processing frame :  43\n",
      "processing frame :  44\n",
      "processing frame :  45\n",
      "processing frame :  46\n",
      "processing frame :  47\n",
      "processing frame :  48\n",
      "processing frame :  49\n",
      "processing frame :  50\n",
      "processing frame :  51\n",
      "processing frame :  52\n",
      "processing frame :  53\n",
      "processing frame :  54\n",
      "processing frame :  55\n",
      "processing frame :  56\n",
      "processing frame :  57\n",
      "processing frame :  58\n",
      "processing frame :  59\n",
      "processing frame :  60\n",
      "processing frame :  61\n",
      "processing frame :  62\n",
      "processing frame :  63\n",
      "processing frame :  64\n",
      "processing frame :  65\n",
      "processing frame :  66\n",
      "processing frame :  67\n",
      "processing frame :  68\n",
      "processing frame :  69\n",
      "processing frame :  70\n",
      "processing frame :  71\n",
      "processing frame :  72\n",
      "processing frame :  73\n",
      "processing frame :  74\n",
      "processing frame :  75\n",
      "processing frame :  76\n",
      "processing frame :  77\n",
      "processing frame :  78\n",
      "processing frame :  79\n",
      "processing frame :  80\n",
      "processing frame :  81\n",
      "processing frame :  82\n",
      "processing frame :  83\n",
      "processing frame :  84\n",
      "processing frame :  85\n",
      "processing frame :  86\n",
      "processing frame :  87\n",
      "processing frame :  88\n",
      "processing frame :  89\n",
      "processing frame :  90\n",
      "processing frame :  91\n",
      "processing frame :  92\n",
      "processing frame :  93\n",
      "processing frame :  94\n",
      "processing frame :  95\n",
      "processing frame :  96\n",
      "processing frame :  97\n",
      "processing frame :  98\n",
      "processing frame :  99\n",
      "processing frame :  100\n",
      "processing frame :  101\n",
      "processing frame :  102\n",
      "processing frame :  103\n",
      "processing frame :  104\n",
      "processing frame :  105\n",
      "processing frame :  106\n",
      "processing frame :  107\n",
      "processing frame :  108\n",
      "processing frame :  109\n",
      "processing frame :  110\n",
      "processing frame :  111\n",
      "processing frame :  112\n",
      "processing frame :  113\n",
      "processing frame :  114\n",
      "processing frame :  115\n",
      "processing frame :  116\n",
      "processing frame :  117\n",
      "processing frame :  118\n",
      "processing frame :  119\n",
      "processing frame :  120\n",
      "processing frame :  121\n",
      "processing frame :  122\n",
      "processing frame :  123\n",
      "processing frame :  124\n",
      "processing frame :  125\n",
      "processing frame :  126\n",
      "processing frame :  127\n",
      "processing frame :  128\n",
      "processing frame :  129\n",
      "processing frame :  130\n",
      "processing frame :  131\n",
      "processing frame :  132\n",
      "processing frame :  133\n",
      "processing frame :  134\n",
      "processing frame :  135\n",
      "processing frame :  136\n",
      "processing frame :  137\n",
      "processing frame :  138\n",
      "processing frame :  139\n",
      "processing frame :  140\n",
      "processing frame :  141\n",
      "processing frame :  142\n",
      "processing frame :  143\n",
      "processing frame :  144\n",
      "processing frame :  145\n",
      "processing frame :  146\n",
      "processing frame :  147\n",
      "processing frame :  148\n",
      "processing frame :  149\n",
      "processing frame :  150\n",
      "processing frame :  151\n",
      "processing frame :  152\n",
      "processing frame :  153\n",
      "processing frame :  154\n",
      "processing frame :  155\n",
      "processing frame :  156\n",
      "processing frame :  157\n",
      "processing frame :  158\n",
      "processing frame :  159\n",
      "processing frame :  160\n",
      "processing frame :  161\n",
      "processing frame :  162\n",
      "processing frame :  163\n",
      "processing frame :  164\n",
      "processing frame :  165\n",
      "processing frame :  166\n",
      "processing frame :  167\n",
      "processing frame :  168\n",
      "processing frame :  169\n",
      "processing frame :  170\n",
      "processing frame :  171\n",
      "processing frame :  172\n",
      "processing frame :  173\n",
      "processing frame :  174\n",
      "processing frame :  175\n",
      "processing frame :  176\n",
      "processing frame :  177\n",
      "processing frame :  178\n",
      "processing frame :  179\n",
      "processing frame :  180\n",
      "processing frame :  181\n",
      "processing frame :  182\n",
      "processing frame :  183\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/skynet/Documents/MEng-Robotics/ENPM673/P5/aruco_DO/interactive_DO.ipynb Cell 20\u001b[0m in \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/skynet/Documents/MEng-Robotics/ENPM673/P5/aruco_DO/interactive_DO.ipynb#X23sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mputText(img, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mc_idx\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, org\u001b[39m=\u001b[39m(x1,y1),\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/skynet/Documents/MEng-Robotics/ENPM673/P5/aruco_DO/interactive_DO.ipynb#X23sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m                         fontFace\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mFONT_HERSHEY_SIMPLEX,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/skynet/Documents/MEng-Robotics/ENPM673/P5/aruco_DO/interactive_DO.ipynb#X23sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m                         fontScale\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/skynet/Documents/MEng-Robotics/ENPM673/P5/aruco_DO/interactive_DO.ipynb#X23sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m                         color\u001b[39m=\u001b[39m(\u001b[39m255\u001b[39m,\u001b[39m255\u001b[39m,\u001b[39m255\u001b[39m),\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/skynet/Documents/MEng-Robotics/ENPM673/P5/aruco_DO/interactive_DO.ipynb#X23sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m                         thickness\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/skynet/Documents/MEng-Robotics/ENPM673/P5/aruco_DO/interactive_DO.ipynb#X23sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m                         lineType\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mLINE_AA)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/skynet/Documents/MEng-Robotics/ENPM673/P5/aruco_DO/interactive_DO.ipynb#X23sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m     img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mrectangle(img, (x1,y1),(x2,y2),color\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m255\u001b[39m,\u001b[39m0\u001b[39m),thickness\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/skynet/Documents/MEng-Robotics/ENPM673/P5/aruco_DO/interactive_DO.ipynb#X23sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m cv2\u001b[39m.\u001b[39;49mimwrite(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(output_image_dir,\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mimage_\u001b[39;49m\u001b[39m{\u001b[39;49;00mframe_idx\u001b[39m}\u001b[39;49;00m\u001b[39m.png\u001b[39;49m\u001b[39m\"\u001b[39;49m),img)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/skynet/Documents/MEng-Robotics/ENPM673/P5/aruco_DO/interactive_DO.ipynb#X23sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m videoWriter\u001b[39m.\u001b[39mwrite(img)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/skynet/Documents/MEng-Robotics/ENPM673/P5/aruco_DO/interactive_DO.ipynb#X23sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39mtry\u001b[39;00m:    \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "frame_idx = 0\n",
    "prev_embeddings = [] # prev frame as 1, next frame as 2\n",
    "next_embeddings = []\n",
    "\n",
    "frame_interval = 10 # after how many frames check similarity\n",
    "avg_processing_time = 0\n",
    "car_idx = 0 #car counts detected cars in the video, same car will be not counted more than 1\n",
    "matching_thresh = 0.7\n",
    "\n",
    "obj_track = {}\n",
    "iou_thresh = 0.7\n",
    "prev_embeddings = []\n",
    "# with vision.ImageEmbedder.create_from_options(options) as embedder:\n",
    "while cap.isOpened():\n",
    "    ret, img = cap.read()\n",
    "    if ret == True:\n",
    "        frame_idx+=1\n",
    "        print(\"processing frame : \",frame_idx)\n",
    "        values  = all_detections[frame_idx]\n",
    "        time_start = time.perf_counter()\n",
    "        # img = store_images[frame_idx]\n",
    "        if len(prev_embeddings)==0:\n",
    "            embed_arr = []\n",
    "            car_idx_arr = []\n",
    "            # take first frame as the reference for starting tracking\n",
    "            for bbox, features in values:\n",
    "                car_idx+=1\n",
    "\n",
    "                # embed_arr.append(embedding_result.embeddings[0])\n",
    "                car_idx_arr.append(car_idx)\n",
    "                obj_track[car_idx] = obj_track.get(car_idx, Track(car_idx,bbox=bbox, features=features))\n",
    "\n",
    "                # print(int(frame_idx//frame_interval),embedding_result)\n",
    "                prev_embeddings.append((features,car_idx,bbox))\n",
    "            # print(obj_track)\n",
    "            # break\n",
    "        else:\n",
    "            for bbox, features in values:\n",
    "                em1 = features\n",
    "                best_match = 0\n",
    "                new_car_idx = None\n",
    "                pe = prev_embeddings.copy()\n",
    "                idx_remove = None\n",
    "                best_iou = 0\n",
    "                best_iou_idx = 0\n",
    "                for idx, (embed_prev, c_idx, coords) in enumerate(prev_embeddings):\n",
    "                    em2 = embed_prev\n",
    "                    similarity = vision.ImageEmbedder.cosine_similarity(\n",
    "                        em1,\n",
    "                        em2)\n",
    "                    \n",
    "                    iou = IOU(bbox, coords)\n",
    "                    if iou>iou_thresh and iou>best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_iou_idx = idx\n",
    "\n",
    "                    if similarity>matching_thresh and similarity>best_match:\n",
    "                        best_match = similarity\n",
    "                        new_car_idx = c_idx\n",
    "                        idx_remove = idx\n",
    "                if new_car_idx is None:\n",
    "                    flag= True\n",
    "                    if best_iou_idx!=0:\n",
    "                        bbox_p = obj_track[best_iou_idx].predict().squeeze().tolist()\n",
    "                        iou = IOU(bbox_p,bbox)\n",
    "                        print(iou)\n",
    "                        if iou>0.95:\n",
    "                            print(\"Got data but embeddings are not matching\")\n",
    "                            # obj_track[best_iou_idx].predict()\n",
    "                            obj_track[best_iou_idx].update(bbox=bbox,features=features)\n",
    "                            flag= False\n",
    "                            new_car_idx = best_iou_idx\n",
    "                            \n",
    "                    if flag:\n",
    "                        car_idx+=1\n",
    "                        new_car_idx = car_idx\n",
    "                        obj_track[car_idx] = obj_track.get(car_idx, Track(car_idx, bbox=bbox, features=features))\n",
    "                else:\n",
    "                    prev_embeddings.pop(idx_remove)\n",
    "                    x1,y1,x2,y2 = bbox\n",
    "                    _ = obj_track[new_car_idx].predict()\n",
    "                    # print(features)\n",
    "                    # print(features.embedding)\n",
    "                    obj_track[new_car_idx].update(bbox=[x1,y1,x2,y2],features=features)\n",
    "                # print(int(frame_idx//frame_interval),embedding_result)\n",
    "                next_embeddings.append((features,new_car_idx,bbox))\n",
    "            # reduce the count of predict for the cars not found\n",
    "            for _,c_id,_ in prev_embeddings:\n",
    "                ret = obj_track[c_id].notFound()\n",
    "                if ret:\n",
    "                    # object not found for certain frames deleting the object\n",
    "                    del obj_track[c_id]\n",
    "            prev_embeddings = next_embeddings.copy()\n",
    "            next_embeddings = []\n",
    "\n",
    "        for embed_prev, c_idx, coords in prev_embeddings:\n",
    "            x1 = coords[0]\n",
    "            y1 = coords[1]\n",
    "            x2 = coords[2]\n",
    "            y2 = coords[3]\n",
    "            img = cv2.rectangle(img, (x1,y1),(x1+3,y1+3),color=(255,0,0),thickness=-1)\n",
    "            img = cv2.putText(img, f'{c_idx}', org=(x1,y1),\n",
    "                                fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                fontScale=3,\n",
    "                                color=(255,255,255),\n",
    "                                thickness=2,\n",
    "                                lineType=cv2.LINE_AA)\n",
    "            img = cv2.rectangle(img, (x1,y1),(x2,y2),color=(0,255,0),thickness=5)\n",
    "        cv2.imwrite(os.path.join(output_image_dir,f\"image_{frame_idx}.png\"),img)\n",
    "        videoWriter.write(img)\n",
    "        try:    \n",
    "            avg_processing_time+=(time.perf_counter()-time_start)/((frame_idx-1)//frame_interval)\n",
    "        except ZeroDivisionError as ZE:\n",
    "            avg_processing_time+=(time.perf_counter()-time_start)\n",
    "\n",
    "    # else:\n",
    "    #     print(\"Not able to read the frame\")\n",
    "    #     break\n",
    "cap.release()\n",
    "videoWriter.release()\n",
    "print(\"time taken : \",avg_processing_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "174",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/skynet/Documents/MEng-Robotics/ENPM673/P5/aruco_DO/interactive_DO.ipynb Cell 20\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/skynet/Documents/MEng-Robotics/ENPM673/P5/aruco_DO/interactive_DO.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m obj_track[\u001b[39m174\u001b[39;49m]\u001b[39m.\u001b[39mpredict()\n",
      "\u001b[0;31mKeyError\u001b[0m: 174"
     ]
    }
   ],
   "source": [
    "obj_track[174].predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: <__main__.Track at 0x7f50d655fca0>,\n",
       " 2: <__main__.Track at 0x7f50ab358310>,\n",
       " 3: <__main__.Track at 0x7f50c866ef40>,\n",
       " 4: <__main__.Track at 0x7f50c866ef10>,\n",
       " 5: <__main__.Track at 0x7f50c866eeb0>,\n",
       " 6: <__main__.Track at 0x7f50c864e040>,\n",
       " 7: <__main__.Track at 0x7f50c864e100>,\n",
       " 8: <__main__.Track at 0x7f50c8686370>,\n",
       " 9: <__main__.Track at 0x7f50cde91b50>,\n",
       " 10: <__main__.Track at 0x7f50cde91b80>,\n",
       " 11: <__main__.Track at 0x7f50cde860d0>,\n",
       " 12: <__main__.Track at 0x7f50cde91f40>,\n",
       " 13: <__main__.Track at 0x7f50cde91e20>,\n",
       " 14: <__main__.Track at 0x7f50cde91bb0>,\n",
       " 15: <__main__.Track at 0x7f50cde91fa0>,\n",
       " 16: <__main__.Track at 0x7f50cde91f70>,\n",
       " 17: <__main__.Track at 0x7f50cde91c40>,\n",
       " 18: <__main__.Track at 0x7f50cde91cd0>,\n",
       " 19: <__main__.Track at 0x7f50cde91b20>,\n",
       " 20: <__main__.Track at 0x7f50cde91be0>,\n",
       " 21: <__main__.Track at 0x7f50cde91d90>,\n",
       " 22: <__main__.Track at 0x7f50cde91970>,\n",
       " 23: <__main__.Track at 0x7f50cde86070>,\n",
       " 24: <__main__.Track at 0x7f50cde91a90>,\n",
       " 25: <__main__.Track at 0x7f50ab963970>,\n",
       " 26: <__main__.Track at 0x7f50ab963fa0>,\n",
       " 27: <__main__.Track at 0x7f50ab963640>,\n",
       " 28: <__main__.Track at 0x7f50ab9637c0>,\n",
       " 29: <__main__.Track at 0x7f50ab9634c0>,\n",
       " 30: <__main__.Track at 0x7f50ab963df0>,\n",
       " 31: <__main__.Track at 0x7f50ab963550>,\n",
       " 32: <__main__.Track at 0x7f50ab9633a0>,\n",
       " 33: <__main__.Track at 0x7f50ab963790>,\n",
       " 34: <__main__.Track at 0x7f50ab963cd0>,\n",
       " 35: <__main__.Track at 0x7f50ab963670>,\n",
       " 36: <__main__.Track at 0x7f50ab963880>,\n",
       " 37: <__main__.Track at 0x7f50ab963be0>,\n",
       " 38: <__main__.Track at 0x7f50ab9635b0>,\n",
       " 39: <__main__.Track at 0x7f50ab963730>,\n",
       " 40: <__main__.Track at 0x7f50ab963ac0>,\n",
       " 41: <__main__.Track at 0x7f50cde917f0>,\n",
       " 42: <__main__.Track at 0x7f50ab963340>,\n",
       " 43: <__main__.Track at 0x7f50cdd94100>,\n",
       " 44: <__main__.Track at 0x7f50cdd940a0>,\n",
       " 45: <__main__.Track at 0x7f50cdd942e0>,\n",
       " 46: <__main__.Track at 0x7f50cdd94040>,\n",
       " 47: <__main__.Track at 0x7f50cdd94340>,\n",
       " 48: <__main__.Track at 0x7f50cdd94250>,\n",
       " 49: <__main__.Track at 0x7f50cdd942b0>,\n",
       " 50: <__main__.Track at 0x7f50cdd94460>,\n",
       " 51: <__main__.Track at 0x7f50cdd944c0>,\n",
       " 52: <__main__.Track at 0x7f50cdd945b0>,\n",
       " 53: <__main__.Track at 0x7f50cdd94490>,\n",
       " 54: <__main__.Track at 0x7f50cdd94700>,\n",
       " 55: <__main__.Track at 0x7f50cdd94640>,\n",
       " 56: <__main__.Track at 0x7f50cdd948b0>,\n",
       " 57: <__main__.Track at 0x7f50cdd947f0>,\n",
       " 58: <__main__.Track at 0x7f50cdd94940>,\n",
       " 59: <__main__.Track at 0x7f50cdd94760>,\n",
       " 60: <__main__.Track at 0x7f50cdd945e0>,\n",
       " 61: <__main__.Track at 0x7f50cdd94af0>,\n",
       " 62: <__main__.Track at 0x7f50cdd94910>,\n",
       " 63: <__main__.Track at 0x7f50cdd949d0>,\n",
       " 64: <__main__.Track at 0x7f50cdd94bb0>,\n",
       " 65: <__main__.Track at 0x7f50cdd94c10>,\n",
       " 66: <__main__.Track at 0x7f50cdd94b50>,\n",
       " 67: <__main__.Track at 0x7f50cdd94850>,\n",
       " 68: <__main__.Track at 0x7f50cdd94d00>,\n",
       " 69: <__main__.Track at 0x7f50cdd94820>,\n",
       " 70: <__main__.Track at 0x7f50cdd94a00>,\n",
       " 71: <__main__.Track at 0x7f50cdd94d60>,\n",
       " 72: <__main__.Track at 0x7f50cdd94dc0>,\n",
       " 73: <__main__.Track at 0x7f50cdd94e80>,\n",
       " 74: <__main__.Track at 0x7f50cdd94df0>,\n",
       " 75: <__main__.Track at 0x7f50cdd94b20>,\n",
       " 76: <__main__.Track at 0x7f50cdd947c0>,\n",
       " 77: <__main__.Track at 0x7f50cdd949a0>,\n",
       " 78: <__main__.Track at 0x7f50cdd94ee0>,\n",
       " 79: <__main__.Track at 0x7f50cdd94fd0>,\n",
       " 80: <__main__.Track at 0x7f50cdd94f70>,\n",
       " 81: <__main__.Track at 0x7f50ab963430>,\n",
       " 82: <__main__.Track at 0x7f50cddaa310>,\n",
       " 83: <__main__.Track at 0x7f50cddaa070>,\n",
       " 84: <__main__.Track at 0x7f50cdd94730>,\n",
       " 85: <__main__.Track at 0x7f50cddaa130>,\n",
       " 86: <__main__.Track at 0x7f50cddaa190>,\n",
       " 87: <__main__.Track at 0x7f50cddaa160>,\n",
       " 88: <__main__.Track at 0x7f50cddaa250>,\n",
       " 89: <__main__.Track at 0x7f50cddaa0a0>,\n",
       " 90: <__main__.Track at 0x7f50cddaa1f0>,\n",
       " 91: <__main__.Track at 0x7f50cddaa340>,\n",
       " 92: <__main__.Track at 0x7f50cddaa370>,\n",
       " 93: <__main__.Track at 0x7f50cddaa7c0>,\n",
       " 94: <__main__.Track at 0x7f50cddaa5b0>,\n",
       " 95: <__main__.Track at 0x7f50cddaaac0>,\n",
       " 96: <__main__.Track at 0x7f50cddaa820>,\n",
       " 97: <__main__.Track at 0x7f50cddaa580>,\n",
       " 98: <__main__.Track at 0x7f50cddaa8b0>,\n",
       " 99: <__main__.Track at 0x7f50cddaa700>,\n",
       " 100: <__main__.Track at 0x7f50cddaa8e0>,\n",
       " 101: <__main__.Track at 0x7f50cddaa550>,\n",
       " 102: <__main__.Track at 0x7f50cddaaa00>,\n",
       " 103: <__main__.Track at 0x7f50cddaab50>}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2338.0, 1684.0, 3045.0, 2170.0]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_track[103].predict().squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
